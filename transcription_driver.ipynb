{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE!\n",
    "\n",
    "Be sure to read the README as part of this Github repo to do the pre-requesite steps in preparing your AWS for \n",
    "usage by this script.\n",
    "\n",
    "**Given AWS calls use real resources all API calls that actually call AWS are initially commented out, take away comments and run cells with knowledge of what they are doing!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.client import ClientError\n",
    "import tscribe\n",
    "import json\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "'''User Entry:\n",
    "\n",
    "Please insert your AWS access key id and AWS secret access key.\n",
    "\n",
    "NOTE: Do NOT push AWS secret access key to Git EVER!\n",
    "'''\n",
    "# AWS Access Information\n",
    "AWS_ACCESS_KEY_ID = \"Enter in AWS Access Key ID\"\n",
    "AWS_SECRET_ACCESS_KEY = \"Enter in AWS Secret Access Key\"\n",
    "\n",
    "# AWS Bucket Information\n",
    "S3_INTERVIEW_BUCKET_NAME = \"\"  # Name of s3 bucket where your interview files are\n",
    "S3_TRANSCRIPT_BUCKET_NAME = \"\" # Name of s3 bucket where your transcript files will be put\n",
    "S3_MODEL_OUTPUT_BUCKET_NAME = \"\"  # Name of s3 bucket where model output will be put\n",
    "\n",
    "# AWS ARN & User Information\n",
    "# ARN code corresponding to custom classifier in AWS (https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html)\n",
    "S3_CUSTOM_CLASSIFIER_MODEL = ''\n",
    "# ARN code corresponding to AWS role that has correct permissions to run classification & entity recognition (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) \n",
    "S3_MODEL_RUNNING_ROLE = ''\n",
    "'''\n",
    "User Entry Area completed.\n",
    "'''\n",
    "\n",
    "# Create needed clients\n",
    "session = boto3.Session(aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "transcribe = boto3.client('transcribe', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, \n",
    "                          region_name=\"us-east-1\")\n",
    "comprehend_client = boto3.client('comprehend', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, \n",
    "                          region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid bucket name \"\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-e2808019544f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ensure all buckets are properly created in S3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0ms3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_bucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS3_INTERVIEW_BUCKET_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# The bucket does not exist or you have no access.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[0;32m    356\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    632\u001b[0m         }\n\u001b[0;32m    633\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[1;32m--> 634\u001b[1;33m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[1;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[0;32m    678\u001b[0m                                  context=None):\n\u001b[0;32m    679\u001b[0m         api_params = self._emit_api_params(\n\u001b[1;32m--> 680\u001b[1;33m             api_params, operation_model, context)\n\u001b[0m\u001b[0;32m    681\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m    682\u001b[0m             api_params, operation_model)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_emit_api_params\u001b[1;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 \u001b[0mservice_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                 operation_name=operation_name),\n\u001b[1;32m--> 712\u001b[1;33m             params=api_params, model=operation_model, context=context)\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Event %s: calling handler %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\botocore\\handlers.py\u001b[0m in \u001b[0;36mvalidate_bucket_name\u001b[1;34m(params, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;34m'Invalid bucket name \"%s\": Bucket name must match '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             'the regex \"%s\"' % (bucket, VALID_BUCKET.pattern))\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid bucket name \"\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\""
     ]
    }
   ],
   "source": [
    "# Ensure all buckets are properly created in S3\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=S3_INTERVIEW_BUCKET_NAME)\n",
    "except ClientError:\n",
    "    # The bucket does not exist or you have no access.\n",
    "    print(f\"ClientError when trying to retreive: {S3_INTERVIEW_BUCKET_NAME}. The bucket does not exist or you have no access.\\n\")\n",
    "    \n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=S3_TRANSCRIPT_BUCKET_NAME)\n",
    "except ClientError:\n",
    "    # The bucket does not exist or you have no access.\n",
    "    print(f\"ClientError when trying to retreive: {S3_TRANSCRIPT_BUCKET_NAME}. The bucket does not exist or you have no access.\\n\")\n",
    "\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=S3_MODEL_OUTPUT_BUCKET_NAME)\n",
    "except ClientError:\n",
    "    # The bucket does not exist or you have no access.\n",
    "    print(f\"ClientError when trying to retreive: {S3_MODEL_OUTPUT_BUCKET_NAME}. The bucket does not exist or you have no access.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Creating Interview JSON Transcriptions\n",
    "\n",
    "Let's create the transcription job(s) for your audio/video interview files. The following cell will gather all of the interview audio files, then subsequently create transcription jobs on AWS to transcribe your audio to JSON formatted transcriptions!\n",
    "\n",
    "### WARNING!\n",
    "The following cell instructs AWS to create transcriptions of your audio files! This means real requests are being made to AWS! As this can cost real resources be careful using this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing file: qual1_par1.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par1.mp3 under job name: qual1_par1\n",
      "Transcribing file: qual1_par2 pt 1.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par2 pt 1.mp3 under job name: qual1_par2pt1\n",
      "Transcribing file: qual1_par2 pt 2.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par2 pt 2.mp3 under job name: qual1_par2pt2\n",
      "Transcribing file: qual1_par3.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par3.mp3 under job name: qual1_par3\n",
      "Transcribing file: qual1_par4 pt 1.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par4 pt 1.mp3 under job name: qual1_par4pt1\n",
      "Transcribing file: qual1_par5.mp3 with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual1_par5.mp3 under job name: qual1_par5\n",
      "Transcribing file: qual2_UC Berkeley 2.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual2_UC Berkeley 2.m4a under job name: qual2_UCBerkeley2\n",
      "Transcribing file: qual2_UC Berkeley 3.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual2_UC Berkeley 3.m4a under job name: qual2_UCBerkeley3\n",
      "Transcribing file: qual2_UC Berkeley 4.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual2_UC Berkeley 4.m4a under job name: qual2_UCBerkeley4\n",
      "Transcribing file: qual2_UC Berkeley.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual2_UC Berkeley.m4a under job name: qual2_UCBerkeley\n",
      "Transcribing file: qual2_University of California.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual2_University of California.m4a under job name: qual2_UniversityofCalifornia\n",
      "Transcribing file: qual3_New Recording 31.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_New Recording 31.m4a under job name: qual3_NewRecording31\n",
      "Transcribing file: qual3_New Recording 32.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_New Recording 32.m4a under job name: qual3_NewRecording32\n",
      "Transcribing file: qual3_Subject 3 Recording.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_Subject 3 Recording.m4a under job name: qual3_Subject3Recording\n",
      "Transcribing file: qual3_Subject 4.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_Subject 4.m4a under job name: qual3_Subject4\n",
      "Transcribing file: qual3_par1 Recording.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_par1 Recording.m4a under job name: qual3_par1Recording\n",
      "Transcribing file: qual3_par2 Recording.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual3_par2 Recording.m4a under job name: qual3_par2Recording\n",
      "Transcribing file: qual4_par1.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par1.m4a under job name: qual4_par1\n",
      "Transcribing file: qual4_par2.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par2.m4a under job name: qual4_par2\n",
      "Transcribing file: qual4_par3.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par3.m4a under job name: qual4_par3\n",
      "Transcribing file: qual4_par4.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par4.m4a under job name: qual4_par4\n",
      "Transcribing file: qual4_par5.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par5.m4a under job name: qual4_par5\n",
      "Transcribing file: qual4_par6.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par6.m4a under job name: qual4_par6\n",
      "Transcribing file: qual4_par7.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/qual4_par7.m4a under job name: qual4_par7\n",
      "Transcribing file: ux1_UX SUB1.m4a with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux1_UX SUB1.m4a under job name: ux1_UXSUB1\n",
      "Transcribing file: ux2_par1.mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux2_par1.mp4 under job name: ux2_par1\n",
      "Transcribing file: ux2_par2.mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux2_par2.mp4 under job name: ux2_par2\n",
      "Transcribing file: ux2_par3.mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux2_par3.mp4 under job name: ux2_par3\n",
      "Transcribing file: ux3_participant_1__all_prototypes_user_test (360p).mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux3_participant_1__all_prototypes_user_test (360p).mp4 under job name: ux3_participant_1__all_prototypes_user_test(360p)\n",
      "Transcribing file: ux3_user_5_user_test (360p).mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux3_user_5_user_test (360p).mp4 under job name: ux3_user_5_user_test(360p)\n",
      "Transcribing file: ux3_user_6__user_test (360p).mp4 with uri: https://infoorginterviewbucket.s3.amazonaws.com/ux3_user_6__user_test (360p).mp4 under job name: ux3_user_6__user_test(360p)\n"
     ]
    }
   ],
   "source": [
    "# Collect all interview audio files\n",
    "media_dict = {}\n",
    "interview_bucket = s3.Bucket(S3_INTERVIEW_BUCKET_NAME)\n",
    "for my_bucket_object in interview_bucket.objects.all():\n",
    "    media_dict[my_bucket_object.key] = f\"https://{S3_INTERVIEW_BUCKET_NAME}.s3.amazonaws.com/{my_bucket_object.key}\"\n",
    "\n",
    "# Loop through all interviews and transcribe them\n",
    "for file, uri in media_dict.items():\n",
    "    job_uri = uri\n",
    "    job_name = (file.split('.')[0]).replace(\" \", \"\")\n",
    "    \n",
    "    print(f\"Transcribing file: {file} with uri: {job_uri} under job name: {job_name}\")\n",
    "#     transcribe.start_transcription_job(\n",
    "#         TranscriptionJobName=job_name,\n",
    "#         Media={'MediaFileUri': job_uri},\n",
    "#         MediaFormat=file.split('.')[1],\n",
    "#         LanguageCode='en-US',\n",
    "#         Settings={'ShowSpeakerLabels': True,'MaxSpeakerLabels': 2},\n",
    "#         OutputBucketName=S3_TRANSCRIPT_BUCKET_NAME\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "After all AWS call cells you should check your AWS portal to see when the jobs are done, then move onto next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.5 Transform JSON Transcripts to .txt\n",
    "\n",
    "Now that the transciption jobs are done we have a collection of transcripts in JSON format. This format is not condusive to analysis so let's convert them to .txt, there is a handy python library (tscribe) that will help us do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize all JSON objects from S3 into local files on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON to file: qual1_par1.json\n",
      "Writing JSON to file: qual1_par2pt1.json\n",
      "Writing JSON to file: qual1_par2pt2.json\n",
      "Writing JSON to file: qual1_par3.json\n",
      "Writing JSON to file: qual1_par4pt1.json\n",
      "Writing JSON to file: qual1_par5.json\n",
      "Writing JSON to file: qual2_UCBerkeley.json\n",
      "Writing JSON to file: qual2_UCBerkeley2.json\n",
      "Writing JSON to file: qual2_UCBerkeley3.json\n",
      "Writing JSON to file: qual2_UCBerkeley4.json\n",
      "Writing JSON to file: qual2_UniversityofCalifornia.json\n",
      "Writing JSON to file: qual3_NewRecording31.json\n",
      "Writing JSON to file: qual3_NewRecording32.json\n",
      "Writing JSON to file: qual3_Subject3Recording.json\n",
      "Writing JSON to file: qual3_Subject4.json\n",
      "Writing JSON to file: qual3_par1Recording.json\n",
      "Writing JSON to file: qual3_par2Recording.json\n",
      "Writing JSON to file: qual4_par1.json\n",
      "Writing JSON to file: qual4_par2.json\n",
      "Writing JSON to file: qual4_par3.json\n",
      "Writing JSON to file: qual4_par4.json\n",
      "Writing JSON to file: qual4_par5.json\n",
      "Writing JSON to file: qual4_par6.json\n",
      "Writing JSON to file: qual4_par7.json\n",
      "Writing JSON to file: ux1_UXSUB1.json\n",
      "Writing JSON to file: ux2_par1.json\n",
      "Writing JSON to file: ux2_par2.json\n",
      "Writing JSON to file: ux2_par3.json\n",
      "Writing JSON to file: ux3_par1.json\n",
      "Writing JSON to file: ux3_user5.json\n",
      "Writing JSON to file: ux3_user6.json\n"
     ]
    }
   ],
   "source": [
    "# Create temporary folder for local transcript saving\n",
    "if not os.path.exists('./temptranscriptfolder'):\n",
    "    os.mkdir('./temptranscriptfolder')\n",
    "\n",
    "# Serialize and save locally to machine\n",
    "transcript_bucket = s3.Bucket(S3_TRANSCRIPT_BUCKET_NAME)\n",
    "for my_bucket_object in transcript_bucket.objects.all():\n",
    "    if 'json' in my_bucket_object.key:\n",
    "        file_name = my_bucket_object.key.split(\".\")[0]\n",
    "        myjson = json.loads(my_bucket_object.get()['Body'].read())\n",
    "        \n",
    "        # Serializing json\n",
    "        json_object = json.dumps(myjson, indent=4)\n",
    "\n",
    "        print(f\"Writing JSON to file: {file_name}.json\")\n",
    "#         # Writing to sample.json\n",
    "#         with open(f\"./temptranscriptfolder/{file_name}.json\", \"w\") as outfile:\n",
    "#             outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take local JSON files and transform them into .vtt, then into .txt for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file qual1_par1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par2pt1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par2pt1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par2pt2.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par2pt2.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par3.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par3.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par4pt1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par4pt1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par5.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual1_par5.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley2.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley2.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley3.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley3.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley4.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UCBerkeley4.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UniversityofCalifornia.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual2_UniversityofCalifornia.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_NewRecording31.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_NewRecording31.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_NewRecording32.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_NewRecording32.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_par1Recording.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_par1Recording.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_par2Recording.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_par2Recording.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_Subject3Recording.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_Subject3Recording.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_Subject4.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual3_Subject4.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par2.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par2.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par3.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par3.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par4.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par4.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par5.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par5.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par6.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par6.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par7.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file qual4_par7.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux1_UXSUB1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux1_UXSUB1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par2.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par2.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par3.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux2_par3.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_par1.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_par1.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_user5.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_user5.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_user6.json to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n",
      "Transforming file ux3_user6.txt to vtt.\n",
      "Transform vtt files to txt for analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON -> .vtt -> .txt\n",
    "for json_file in os.listdir('./temptranscriptfolder'):\n",
    "    file_name = json_file.split(\".\")[0]\n",
    "    print(f\"Transforming file {json_file} to vtt.\")\n",
    "#     transcription_base_file_name = f\"./temptranscriptfolder/{json_file}\"\n",
    "#     tscribe.write(f\"./temptranscriptfolder/{json_file}\", save_as=f\"./temptranscriptfolder/{file_name}\", format=\"vtt\")\n",
    "    print(f\"Transform vtt files to txt for analysis.\\n\")\n",
    "#     p = Path(f'./temptranscriptfolder/{file_name}.vtt')\n",
    "#     p.rename(p.with_suffix('.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload .txt files back to S3 transcript bucket, preparing them for classification and entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./temptranscriptfolder/qual1_par1.txt to bucket infororgtranscriptbucket as object_name: qual1_par1\n",
      "Uploading ./temptranscriptfolder/qual1_par2pt1.txt to bucket infororgtranscriptbucket as object_name: qual1_par2pt1\n",
      "Uploading ./temptranscriptfolder/qual1_par2pt2.txt to bucket infororgtranscriptbucket as object_name: qual1_par2pt2\n",
      "Uploading ./temptranscriptfolder/qual1_par3.txt to bucket infororgtranscriptbucket as object_name: qual1_par3\n",
      "Uploading ./temptranscriptfolder/qual1_par4pt1.txt to bucket infororgtranscriptbucket as object_name: qual1_par4pt1\n",
      "Uploading ./temptranscriptfolder/qual1_par5.txt to bucket infororgtranscriptbucket as object_name: qual1_par5\n",
      "Uploading ./temptranscriptfolder/qual2_UCBerkeley.txt to bucket infororgtranscriptbucket as object_name: qual2_UCBerkeley\n",
      "Uploading ./temptranscriptfolder/qual2_UCBerkeley2.txt to bucket infororgtranscriptbucket as object_name: qual2_UCBerkeley2\n",
      "Uploading ./temptranscriptfolder/qual2_UCBerkeley3.txt to bucket infororgtranscriptbucket as object_name: qual2_UCBerkeley3\n",
      "Uploading ./temptranscriptfolder/qual2_UCBerkeley4.txt to bucket infororgtranscriptbucket as object_name: qual2_UCBerkeley4\n",
      "Uploading ./temptranscriptfolder/qual2_UniversityofCalifornia.txt to bucket infororgtranscriptbucket as object_name: qual2_UniversityofCalifornia\n",
      "Uploading ./temptranscriptfolder/qual3_NewRecording31.txt to bucket infororgtranscriptbucket as object_name: qual3_NewRecording31\n",
      "Uploading ./temptranscriptfolder/qual3_NewRecording32.txt to bucket infororgtranscriptbucket as object_name: qual3_NewRecording32\n",
      "Uploading ./temptranscriptfolder/qual3_par1Recording.txt to bucket infororgtranscriptbucket as object_name: qual3_par1Recording\n",
      "Uploading ./temptranscriptfolder/qual3_par2Recording.txt to bucket infororgtranscriptbucket as object_name: qual3_par2Recording\n",
      "Uploading ./temptranscriptfolder/qual3_Subject3Recording.txt to bucket infororgtranscriptbucket as object_name: qual3_Subject3Recording\n",
      "Uploading ./temptranscriptfolder/qual3_Subject4.txt to bucket infororgtranscriptbucket as object_name: qual3_Subject4\n",
      "Uploading ./temptranscriptfolder/qual4_par1.txt to bucket infororgtranscriptbucket as object_name: qual4_par1\n",
      "Uploading ./temptranscriptfolder/qual4_par2.txt to bucket infororgtranscriptbucket as object_name: qual4_par2\n",
      "Uploading ./temptranscriptfolder/qual4_par3.txt to bucket infororgtranscriptbucket as object_name: qual4_par3\n",
      "Uploading ./temptranscriptfolder/qual4_par4.txt to bucket infororgtranscriptbucket as object_name: qual4_par4\n",
      "Uploading ./temptranscriptfolder/qual4_par5.txt to bucket infororgtranscriptbucket as object_name: qual4_par5\n",
      "Uploading ./temptranscriptfolder/qual4_par6.txt to bucket infororgtranscriptbucket as object_name: qual4_par6\n",
      "Uploading ./temptranscriptfolder/qual4_par7.txt to bucket infororgtranscriptbucket as object_name: qual4_par7\n",
      "Uploading ./temptranscriptfolder/ux1_UXSUB1.txt to bucket infororgtranscriptbucket as object_name: ux1_UXSUB1\n",
      "Uploading ./temptranscriptfolder/ux2_par1.txt to bucket infororgtranscriptbucket as object_name: ux2_par1\n",
      "Uploading ./temptranscriptfolder/ux2_par2.txt to bucket infororgtranscriptbucket as object_name: ux2_par2\n",
      "Uploading ./temptranscriptfolder/ux2_par3.txt to bucket infororgtranscriptbucket as object_name: ux2_par3\n",
      "Uploading ./temptranscriptfolder/ux3_par1.txt to bucket infororgtranscriptbucket as object_name: ux3_par1\n",
      "Uploading ./temptranscriptfolder/ux3_user5.txt to bucket infororgtranscriptbucket as object_name: ux3_user5\n",
      "Uploading ./temptranscriptfolder/ux3_user6.txt to bucket infororgtranscriptbucket as object_name: ux3_user6\n"
     ]
    }
   ],
   "source": [
    "# Upload .txt files to S3\n",
    "for file in os.listdir('./temptranscriptfolder'):\n",
    "    if '.txt' in file:\n",
    "        file_path = f'./temptranscriptfolder/{file}'\n",
    "        try:\n",
    "            print(f\"Uploading {file_path} to bucket {S3_TRANSCRIPT_BUCKET_NAME} as object_name: {file.split('.')[0]}\")\n",
    "#             response = s3_client.upload_file(file_path, S3_TRANSCRIPT_BUCKET_NAME, file.split('.')[0])\n",
    "        except ClientError as e:\n",
    "            logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Run Entity Recognition on Transcript .txt files\n",
    "Now that we have all our transcripts in the desire format, we can run classification and entity recognition on them! \n",
    "\n",
    "It is prefered to run the classificaion with a custom model that is more tuned for the interviews you do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running custom classification on MyTranscriptionJob_Kevin.txt\n",
      "Running entity recognition on MyTranscriptionJob_Kevin.txt\n",
      "Running custom classification on qual1_par2pt1.txt\n",
      "Running entity recognition on qual1_par2pt1.txt\n",
      "Running custom classification on qual1_par3.txt\n",
      "Running entity recognition on qual1_par3.txt\n",
      "Running custom classification on qual1_par4pt1.txt\n",
      "Running entity recognition on qual1_par4pt1.txt\n",
      "Running custom classification on qual1_par5.txt\n",
      "Running entity recognition on qual1_par5.txt\n",
      "Running custom classification on qual2_UCBerkeley2.txt\n",
      "Running entity recognition on qual2_UCBerkeley2.txt\n",
      "Running custom classification on qual2_UCBerkeley3.txt\n",
      "Running entity recognition on qual2_UCBerkeley3.txt\n",
      "Running custom classification on qual2_UCBerkeley4.txt\n",
      "Running entity recognition on qual2_UCBerkeley4.txt\n",
      "Running custom classification on qual2_UniversityofCalifornia.txt\n",
      "Running entity recognition on qual2_UniversityofCalifornia.txt\n",
      "Running custom classification on qual3_NewRecording31.txt\n",
      "Running entity recognition on qual3_NewRecording31.txt\n",
      "Running custom classification on qual3_Subject3Recording.txt\n",
      "Running entity recognition on qual3_Subject3Recording.txt\n",
      "Running custom classification on qual3_Subject4.txt\n",
      "Running entity recognition on qual3_Subject4.txt\n",
      "Running custom classification on qual3_par1Recording.txt\n",
      "Running entity recognition on qual3_par1Recording.txt\n",
      "Running custom classification on qual4_par1.txt\n",
      "Running entity recognition on qual4_par1.txt\n",
      "Running custom classification on qual4_par3.txt\n",
      "Running entity recognition on qual4_par3.txt\n",
      "Running custom classification on qual4_par4.txt\n",
      "Running entity recognition on qual4_par4.txt\n",
      "Running custom classification on qual4_par5.txt\n",
      "Running entity recognition on qual4_par5.txt\n",
      "Running custom classification on qual4_par6.txt\n",
      "Running entity recognition on qual4_par6.txt\n",
      "Running custom classification on qual4_par7.txt\n",
      "Running entity recognition on qual4_par7.txt\n",
      "Running custom classification on ux2_par1.txt\n",
      "Running entity recognition on ux2_par1.txt\n",
      "Running custom classification on ux2_par2.txt\n",
      "Running entity recognition on ux2_par2.txt\n",
      "Running custom classification on ux3_par1.txt\n",
      "Running entity recognition on ux3_par1.txt\n",
      "Running custom classification on ux3_user6.txt\n",
      "Running entity recognition on ux3_user6.txt\n"
     ]
    }
   ],
   "source": [
    "# Create Output URI\n",
    "output_uri = f\"s3://{S3_MODEL_OUTPUT_BUCKET_NAME}\"\n",
    "\n",
    "# Fill this in with file names that were used for training as they should not have analysis ran on them\n",
    "training_list = []\n",
    "\n",
    "# Run Classification and Entity Recognition on all transcripts\n",
    "transcript_bucket = s3.Bucket(S3_TRANSCRIPT_BUCKET_NAME)\n",
    "for my_bucket_object in transcript_bucket.objects.all():\n",
    "    if '.txt' in my_bucket_object.key and my_bucket_object.key.split('.')[0] not in training_list:\n",
    "        uri = f\"s3://{S3_TRANSCRIPT_BUCKET_NAME}/{my_bucket_object.key}\"\n",
    "        \n",
    "        print(f\"Running custom classification on {my_bucket_object.key}\")\n",
    "        # Run custom classification \n",
    "#         response = comprehend_client.start_document_classification_job(\n",
    "#             JobName=f\"{my_bucket_object.key.split('.')[0]}\",\n",
    "#             InputDataConfig={\n",
    "#                 'S3Uri': uri,\n",
    "#                 'InputFormat': 'ONE_DOC_PER_FILE'\n",
    "#             },\n",
    "#             OutputDataConfig={\n",
    "#                 'S3Uri': output_uri,\n",
    "#             },\n",
    "#             DataAccessRoleArn=S3_MODEL_RUNNING_ROLE,\n",
    "#             DocumentClassifierArn=S3_CUSTOM_CLASSIFIER_MODEL\n",
    "#         )\n",
    "        \n",
    "        print(f\"Running entity recognition on {my_bucket_object.key}\")\n",
    "        # Run entity recognition\n",
    "#         response = comprehend_client.start_entities_detection_job(\n",
    "#             JobName=f\"{my_bucket_object.key.split('.')[0]}\",\n",
    "#             InputDataConfig={\n",
    "#                 'S3Uri': uri,\n",
    "#                 'InputFormat': 'ONE_DOC_PER_FILE'\n",
    "#             },\n",
    "#             OutputDataConfig={\n",
    "#                 'S3Uri': output_uri,\n",
    "#             },\n",
    "#             LanguageCode='en',\n",
    "#             DataAccessRoleArn=S3_MODEL_RUNNING_ROLE,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Analyze Entity Recognition Output\n",
    "Now that all those analysis jobs are done we can now analyze the output! \n",
    "\n",
    "This should be a playground for whatever you may need to do but provided is some code that will retreive the model output, then output the classifier results and the top 5 results that came from the entity recognition. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model analysis file: 422470722668-CLN-1454d307e34c6bf8c704669f48f6b5b5/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-2ec7daabfbf98953683f59d9c93043c2/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-30acc617bcedfbb26d0d42939523a90c/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-3481a24f297e4cf2ba227a36bca1b191/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-3e5f12be76914ccdd26ec24c06ef25f9/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-4bc830308cffb42c638c89bfe1e12f9e/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-502df2637b374a17a8710ec5da26957c/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-5128235f3ca8c5390e7ec1f6210b3c9c/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-5fadb5ed8deb762853e51f1ecba13bff/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-6349b5318ad91aaeaf2ea731d1351a3f/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-6a5042fac91df2da90eb6ab7bc62fe78/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-77a6d96bb92b23ab4ccb333bebeb1445/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-910a550818a2bb0a85ed65627d1cf0ba/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-91b63642e6f6b313595757de67616390/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-9501bc51c4d78a7567c4b5657e68756d/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-9ad72b01abcb34ceb811b4d02be7b96c/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-a753561bad13737df4284481ef74feaf/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-a8204cac1df216b62eed48dd178ca02e/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-ac24f055f463223aee301d34ad7c6780/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-ae58862474212d6342726f495bfab364/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-b17f778d9d4bf3af70d3bb75cc82bde0/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-c16e909db044a5f3661d01236bb92e81/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-CLN-e2e3e5e23a2e5ecb7e1b3da5492703c9/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-03676e53aa53b2e6a6d0f2970f3ea6e3/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-046cc6f2ffe02add5f778621767348dd/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-1d8f112bd58dc6201df9437c3e9f5c0f/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-2cc48911d5bd65c863faa02f35288f19/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-3239765050411c1e29e1bf887392ad78/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-3659b5007395d6eefcfcd9176ce4b3f0/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-3f29449a86b355c0ea034c64cce49b06/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-438ba9342bc670f2750440d1c5aaf3f0/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-4d17618234f1f079d4e5814b06064b54/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-4f236b64f02e07db9f5f0bd0af764599/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-4f4fa734942148c70bfdaabc81aa725e/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-4fb5de55e38cfe4b12008806911cc134/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-60a1d201c1880441ddf5fc13737d8021/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-69aaa34adf3538c936f22abaf5240f01/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-6da0275926e480389ca306f452232fa8/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-70d42a9d6c24613d4cd1b45d89f5f928/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-78beee30b8c0b17291fcb2f9dd126131/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-794bfc9dbff6f4cc5753deb0eee11f5d/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-9118443f238fc660606a093dc5c1210d/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-a01f80045fa094530e25dc1806b051af/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-a10e3b48ee9f9a89f286d587bcd42a60/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-c618a5d8678ffc9fec482d101ea5829f/output/output.tar.gz\n",
      "Downloading model analysis file: 422470722668-NER-febb6578ecb600aa8df96912faf17bb3/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Create temporary folder for local saving of model output\n",
    "if not os.path.exists('temptargzfolder'):\n",
    "    os.mkdir('temptargzfolder')\n",
    "\n",
    "model_output_bucket = s3.Bucket(S3_MODEL_OUTPUT_BUCKET_NAME)\n",
    "for my_bucket_object in model_output_bucket.objects.all():\n",
    "    if 'tar.gz' in my_bucket_object.key:\n",
    "        print(f\"Downloading model analysis file: {my_bucket_object.key}\")\n",
    "#         s3_client.download_file(S3_MODEL_OUTPUT_BUCKET_NAME, my_bucket_object.key, \"./temptargzfolder/s3object.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing model output for file: qual1_par3.txt...\n",
      "Analyzing model output for file: qual3_Subject3Recording.txt...\n",
      "Analyzing model output for file: qual4_par3.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley4.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley3.txt...\n",
      "Analyzing model output for file: MyTranscriptionJob_Kevin.txt...\n",
      "Analyzing model output for file: qual1_par5.txt...\n",
      "Analyzing model output for file: qual2_UniversityofCalifornia.txt...\n",
      "Analyzing model output for file: ux3_par1.txt...\n",
      "Analyzing model output for file: qual4_par7.txt...\n",
      "Analyzing model output for file: qual3_par1Recording.txt...\n",
      "Analyzing model output for file: MyTranscriptionJob_Kevin.txt...\n",
      "Analyzing model output for file: ux2_par1.txt...\n",
      "Analyzing model output for file: qual4_par1.txt...\n",
      "Analyzing model output for file: qual4_par6.txt...\n",
      "Analyzing model output for file: qual1_par4pt1.txt...\n",
      "Analyzing model output for file: ux3_user6.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley3.txt...\n",
      "Analyzing model output for file: qual3_Subject3Recording.txt...\n",
      "Analyzing model output for file: qual3_Subject4.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley2.txt...\n",
      "Analyzing model output for file: qual3_par1Recording.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley2.txt...\n",
      "Analyzing model output for file: qual1_par3.txt...\n",
      "Analyzing model output for file: MyTranscriptionJob_Kevin.txt...\n",
      "Analyzing model output for file: qual2_UniversityofCalifornia.txt...\n",
      "Analyzing model output for file: ux3_par1.txt...\n",
      "Analyzing model output for file: MyTranscriptionJob_Kevin.txt...\n",
      "Analyzing model output for file: ux2_par1.txt...\n",
      "Analyzing model output for file: qual4_par7.txt...\n",
      "Analyzing model output for file: qual1_par2pt1.txt...\n",
      "Analyzing model output for file: qual4_par1.txt...\n",
      "Analyzing model output for file: qual3_NewRecording31.txt...\n",
      "Analyzing model output for file: qual3_NewRecording31.txt...\n",
      "Analyzing model output for file: qual4_par5.txt...\n",
      "Analyzing model output for file: qual4_par3.txt...\n",
      "Analyzing model output for file: qual1_par5.txt...\n",
      "Analyzing model output for file: qual4_par4.txt...\n",
      "Analyzing model output for file: qual1_par4pt1.txt...\n",
      "Analyzing model output for file: qual2_UCBerkeley4.txt...\n",
      "Analyzing model output for file: qual4_par6.txt...\n",
      "Analyzing model output for file: qual4_par5.txt...\n",
      "Analyzing model output for file: qual4_par4.txt...\n",
      "Analyzing model output for file: qual1_par2pt1.txt...\n",
      "Analyzing model output for file: qual3_Subject4.txt...\n",
      "Analyzing model output for file: ux3_user6.txt...\n",
      "\n",
      "\n",
      "Analysis for qual1_par3.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.9514}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0271}, {'Name': 'UX QUESTION', 'Score': 0.0099}]\n",
      "Top Entities: [(0.9987548615690072, 'Costco'), (0.9985037812286311, 'Canada'), (0.9973307760848908, 'Iran'), (0.9973068246935975, 'Berkeley'), (0.9968051786795659, 'Shantha')]\n",
      "\n",
      "Analysis for qual3_Subject3Recording.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.5851}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.3345}, {'Name': 'ANECDOTE', 'Score': 0.0397}]\n",
      "Top Entities: [(0.9976032495681714, '15 images'), (0.9961010243688678, 'Kimberly'), (0.9837492067128686, 'two friends'), (0.9825025065898496, 'three parts'), (0.9794347296363903, 'first question')]\n",
      "\n",
      "Analysis for qual4_par3.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.5512}, {'Name': 'ANECDOTE', 'Score': 0.394}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.0295}]\n",
      "Top Entities: [(0.9992227639789955, 'Seattle'), (0.9988483355759515, 'Bob'), (0.9987969583756627, 'California'), (0.9983011782532183, 'Walmart'), (0.9977520093132182, 'Walmart')]\n",
      "\n",
      "Analysis for qual2_UCBerkeley4.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.6477}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.2895}, {'Name': 'ANECDOTE', 'Score': 0.0308}]\n",
      "Top Entities: [(0.9985848455098753, 'Berkeley'), (0.9983250585729606, 'US'), (0.9982478379133993, 'Berkeley'), (0.9977368193228723, 'US'), (0.9974965697931402, 'Africa')]\n",
      "\n",
      "Analysis for qual2_UCBerkeley3.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.8944}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0705}, {'Name': 'UX QUESTION', 'Score': 0.0127}]\n",
      "Top Entities: [(0.9990233194153075, 'India'), (0.9986804278202487, 'India'), (0.9984236809291115, 'India'), (0.9982421359039133, 'Pune'), (0.9980471198504026, 'India')]\n",
      "\n",
      "Analysis for MyTranscriptionJob_Kevin.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.6372}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.2492}, {'Name': 'ANECDOTE', 'Score': 0.0628}]\n",
      "Top Entities: [(0.9994751586926784, 'Kevin'), (0.9986336984519515, 'Street Fighter'), (0.9985305643599524, 'Street Fighter'), (0.9984797486332951, 'Street Fighter'), (0.9965490540333755, 'North Carolina')]\n",
      "\n",
      "Analysis for qual1_par5.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.9456}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0313}, {'Name': 'UX QUESTION', 'Score': 0.0099}]\n",
      "Top Entities: [(0.9993807338480543, 'BSC'), (0.9991026831303539, 'Costco'), (0.9989074497127689, '2012'), (0.9985678470303071, '1984'), (0.9985573868038139, 'Berkeley')]\n",
      "\n",
      "Analysis for qual2_UniversityofCalifornia.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.6061}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.2958}, {'Name': 'ANECDOTE', 'Score': 0.0596}]\n",
      "Top Entities: [(0.9994062136892078, 'Linda'), (0.999247164458953, 'two'), (0.9991908665303846, 'US'), (0.9990662718066795, 'Linda'), (0.9990312909346835, 'Rwanda')]\n",
      "\n",
      "Analysis for ux3_par1.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.9037}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.0416}, {'Name': 'ANECDOTE', 'Score': 0.0353}]\n",
      "Top Entities: [(0.9989805365412354, 'Las Vegas'), (0.9987907744108815, 'Aaron'), (0.9987553857450341, 'Las Vegas'), (0.9986919606990325, 'Tomas'), (0.9976806914479648, 'February')]\n",
      "\n",
      "Analysis for qual4_par7.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.925}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0433}, {'Name': 'UX QUESTION', 'Score': 0.0132}]\n",
      "Top Entities: [(0.9992706138933841, 'Berkeley'), (0.9991385020508033, 'Oakland'), (0.9991143448160522, 'two'), (0.9990536594217889, 'Francis'), (0.9990190362683822, 'Sacramento')]\n",
      "\n",
      "Analysis for qual3_par1Recording.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.6267}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.2981}, {'Name': 'ANECDOTE', 'Score': 0.046}]\n",
      "Top Entities: [(0.9995873827932054, 'Travis Stevens'), (0.9994007365906931, 'Tyson'), (0.9993454630207194, 'Travis Stevens'), (0.9989728226439172, 'Lex Friedman'), (0.998388982616125, 'China')]\n",
      "\n",
      "Analysis for ux2_par1.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.4293}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.3508}, {'Name': 'ANECDOTE', 'Score': 0.1483}]\n",
      "Top Entities: [(0.9665753495955063, 'first page'), (0.9530052919565167, 'English'), (0.8908235057910391, '00:05:05.000'), (0.8846211837598179, '00:03:30.000'), (0.8670429243614745, '00:12:20.000')]\n",
      "\n",
      "Analysis for qual4_par1.txt:\n",
      "Classification: [{'Name': 'ANECDOTE', 'Score': 0.5568}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.393}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.0121}]\n",
      "Top Entities: [(0.9977504131314314, 'six\\nmonths'), (0.9948721429545444, 'three months'), (0.9941928295139881, '5.5 years'), (0.9939676791539527, 'three months'), (0.993282801979782, 'Larry')]\n",
      "\n",
      "Analysis for qual4_par6.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.9179}, {'Name': 'ANECDOTE', 'Score': 0.0404}, {'Name': 'PERSONAL OPINION', 'Score': 0.0106}]\n",
      "Top Entities: [(0.9997038526352292, 'Jason'), (0.9989824074942683, 'San Francisco'), (0.9989552694676269, 'Berkeley'), (0.9987087977936919, '4.5 miles'), (0.9986497539197589, 'Andrea')]\n",
      "\n",
      "Analysis for qual1_par4pt1.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.7572}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.1429}, {'Name': 'ANECDOTE', 'Score': 0.0614}]\n",
      "Top Entities: [(0.9993038259049012, 'Berkeley'), (0.9992884695493932, '2020'), (0.9992684712640226, '2019'), (0.9981442552274105, '30 minute'), (0.9978780421803165, 'Los Angeles')]\n",
      "\n",
      "Analysis for ux3_user6.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.4293}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.3508}, {'Name': 'ANECDOTE', 'Score': 0.1483}]\n",
      "Top Entities: [(0.9665753495955063, 'first page'), (0.9530052919565167, 'English'), (0.8908235057910391, '00:05:05.000'), (0.8846211837598179, '00:03:30.000'), (0.8670429243614745, '00:12:20.000')]\n",
      "\n",
      "Analysis for qual3_Subject4.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.8931}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0602}, {'Name': 'UX QUESTION', 'Score': 0.0179}]\n",
      "Top Entities: [(0.9980442892829293, '15 images'), (0.9968302903225188, 'today'), (0.9965596947396212, '1998'), (0.994546986484043, 'today'), (0.992564470502615, 'today')]\n",
      "\n",
      "Analysis for qual2_UCBerkeley2.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.6175}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.3081}, {'Name': 'ANECDOTE', 'Score': 0.0417}]\n",
      "Top Entities: [(0.9997658086034802, 'Katia'), (0.9995045733565407, 'Taiwan'), (0.9991506405866243, 'Japan'), (0.9990956624570213, 'Japan'), (0.9988867530816945, 'Japan')]\n",
      "\n",
      "Analysis for qual1_par2pt1.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.6594}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.192}, {'Name': 'ANECDOTE', 'Score': 0.0957}]\n",
      "Top Entities: [(0.9995410181958247, 'Betsy Putnam'), (0.9994468174748747, 'NASCA'), (0.9994183587351911, '1944'), (0.9991402870810099, 'NSCO'), (0.999021559497112, '20 properties')]\n",
      "\n",
      "Analysis for qual3_NewRecording31.txt:\n",
      "Classification: [{'Name': 'ANECDOTE', 'Score': 0.9917}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0032}, {'Name': 'PERSONAL OPINION', 'Score': 0.0018}]\n",
      "Top Entities: [(0.999435505204064, '1990'), (0.9993894133088275, 'Jean Claude Van Damme'), (0.9984878029076257, '10 years'), (0.997710356564857, 'America'), (0.9974739152095881, 'Tashiro')]\n",
      "\n",
      "Analysis for qual4_par5.txt:\n",
      "Classification: [{'Name': 'SHARING KNOWLEDGE', 'Score': 0.611}, {'Name': 'ANECDOTE', 'Score': 0.3302}, {'Name': 'QUALITATIVE QUESTION', 'Score': 0.0144}]\n",
      "Top Entities: [(0.9993571601955027, 'Hanna'), (0.999337159779261, 'January 2020'), (0.9979358696151989, 'Department of Public Health'), (0.9977031226713229, 'two people'), (0.9959852403724814, 'tonight')]\n",
      "\n",
      "Analysis for qual4_par4.txt:\n",
      "Classification: [{'Name': 'QUALITATIVE QUESTION', 'Score': 0.9227}, {'Name': 'SHARING KNOWLEDGE', 'Score': 0.0464}, {'Name': 'UX QUESTION', 'Score': 0.013}]\n",
      "Top Entities: [(0.9992377612133292, 'Peter'), (0.9987111035930211, 'NIH'), (0.9986100469765289, 'Novartis'), (0.9980129228291091, 'NIH'), (0.9970779223760182, '2526')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read .tar.gz files and find entry types\n",
    "interview_dict = {}\n",
    "for zipped_file in os.listdir(\"./temptargzfolder/\"):\n",
    "    tar = tarfile.open(f\"./temptargzfolder/{zipped_file}\", \"r:gz\")\n",
    "    for member in tar.getmembers():\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            content = f.read()\n",
    "            decoded_content = json.loads(content.decode('utf-8'))\n",
    "            source_file = decoded_content['File']\n",
    "            if source_file not in interview_dict:\n",
    "                interview_dict[source_file] = {\n",
    "                    'Top Entities': None,\n",
    "                    'Classification': None\n",
    "                }\n",
    "            \n",
    "            print(f\"Analyzing model output for file: {source_file}...\")\n",
    "            if 'Entities' in decoded_content.keys():\n",
    "                entry_list = []\n",
    "                for entry in decoded_content['Entities']:\n",
    "                    entry_list.append((entry['Score'], entry['Text']))\n",
    "                entry_list.sort(reverse=True)\n",
    "                interview_dict[source_file]['Top Entities'] = entry_list[:5]\n",
    "            else:\n",
    "                interview_dict[source_file]['Classification'] = decoded_content['Classes']\n",
    "print(\"\\n\")\n",
    "\n",
    "for file, data in interview_dict.items():\n",
    "    print(f\"Analysis for {file}:\")\n",
    "    print(f\"Classification: {data['Classification']}\")\n",
    "    print(f\"Top Entities: {data['Top Entities']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
